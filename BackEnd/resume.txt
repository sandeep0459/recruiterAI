PRANES GOPALAN VENKATESH
Texas • +1 (551) 786-1731 • pranesgvenkatesh20@gmail.com • LinkedIn • GitHub • Tableau
Professional Summary
I am an experienced AI & Data Science Engineer specializing in large language models (LLMs), RAG pipelines, and prompt evaluation frameworks. My expertise lies in orchestrating AI workflows using LangChain and LangGraph with FastAPI deployments across cloud-native architectures. I have built a strong foundation in Python, NLP (utilizing NLTK, Hugging Face, and SpaCy), and data analytics with visualization tools including Tableau and Power BI. Throughout my career, I've focused on developing scalable machine learning solutions and integrating LLMs into practical applications with structured evaluation methodologies, fine-tuning capabilities, and enterprise-grade reliability.
Technical Skills
My programming proficiency spans Python, R, JavaScript, and SQL. I work extensively with AI/ML frameworks including LangChain, LangGraph, PySpark, TensorFlow, and scikit-learn. For natural language processing tasks, I leverage NLTK, SpaCy, and Hugging Face Transformers. My data visualization toolkit includes Plotly, Seaborn, Matplotlib, and I'm comfortable building web applications using FastAPI, React.js, Tailwind CSS, Node.js, and Express.js.
I have hands-on experience with various ML and AI systems, particularly working with OpenAI API (GPT-3.5, GPT-4), Gemini 2.0 API, implementing RAG pipelines, and conducting prompt tuning and evaluation through LangSmith. For data storage and retrieval, I utilize PostgreSQL, MySQL, MongoDB, Firebase, Snowflake, and vector databases like Pinecone and FAISS. My cloud infrastructure knowledge encompasses AWS S3, Lambda, Google Cloud Storage, Firebase, and Office 365 integrations.
My DevOps and deployment experience includes Apache Airflow for workflow orchestration, GitHub Actions for CI/CD pipelines, and containerization with Docker. I've deployed applications on various platforms including Render, Vercel, and Heroku. For LLM evaluation, I implement both automated metrics through Hugging Face Evaluate and structured manual evaluation frameworks that incorporate human feedback and semantic relevance scoring.
Professional Experience
Since April 2024, I've been working as a Data Scientist at Highbrow - Turing remotely in the United States. In this role, I lead Reinforcement Learning from Human Feedback (RLHF) initiatives for Google, enhancing LLM alignment through iterative feedback processes and behavior tuning to ensure safe, high-quality outputs. I've developed production-grade LangChain and LangGraph pipelines that power dynamic, context-aware LLM orchestration through prompt routing, semantic query understanding, and Retrieval-Augmented Generation (RAG).
A significant part of my work involves designing and implementing structured prompt evaluations to systematically test variations for accuracy, relevance, and safety, driving empirical improvements in model performance. I've used internal labeling tools to assess thousands of LLM completions, providing comparative judgments that inform reward modeling and fine-tuning cycles. Additionally, I contribute to large-scale model benchmarking and dataset curation by analyzing response patterns across various tasks, supporting alignment and annotation-guided training workflows. I regularly collaborate with cross-functional teams to define evaluation criteria, build scalable testing frameworks, and document model behaviors for downstream LLM applications.
Previously, from August 2023 to April 2024, I worked as a Data Analyst at Lorhan Corporation in New Jersey. There, I built and maintained real-time dashboards using Tableau and Power BI to provide business insights for strategic decision-making. I improved data retrieval efficiency by 5% through PostgreSQL query optimization and evaluated different database solutions to support diverse internal data needs. I also automated recurring data processing tasks in Excel using advanced functions and scripting, which reduced manual workload by 40%. I contributed to internal analytics tooling and pre-sales enablement efforts by aligning data infrastructure with evolving client requirements. Working closely with backend engineers and product leads, I helped deploy LLM components on Google Cloud, ensuring seamless integration with internal systems while maintaining low-latency inference.
Projects
One of my key projects is a Resume AI Chatbot built with FastAPI, LangChain, OpenAI, FAISS, React.js, and Tailwind CSS, deployed on Render. This AI assistant answers recruiter questions using Retrieval-Augmented Generation (RAG) with LangChain, FAISS, and OpenAI embeddings. I deployed a FastAPI backend on Render and created an interactive user experience with a React.js frontend.
I also developed a Hotel Review Sentiment Analysis system using Python, Selenium, NLTK, scikit-learn, Gensim, VADER, Doc2Vec, TF-IDF, and visualization libraries. This project involved extracting customer reviews from Airbnb using Selenium and labeling them based on rating thresholds. I performed advanced NLP preprocessing and feature engineering to train classification models for binary sentiment prediction.
Another significant project was a Twitter Data ETL Pipeline implemented with Apache Airflow and AWS integration. I built an automated pipeline to extract real-time Twitter data via API calls, orchestrated using Apache Airflow for scheduled processing. The system leveraged AWS EC2 for compute efficiency and stored processed data on S3 to enable scalable downstream analytics and ML integration.
Additionally, I conducted a King County Real Estate Data Analysis and Visualization project using Excel and Tableau. This involved cleaning and validating real estate sales data in Excel before developing interactive Tableau dashboards to analyze price trends, sales distribution, and investment hotspots in the Washington housing market.
Education
I earned my Master of Science in Data Science from Stevens Institute of Technology in Hoboken, United States, from August 2021 to May 2023, achieving a GPA of 3.8/4.0. My relevant coursework included Applied Machine Learning, Deep Learning, Web Mining, Big Data, DBMS, and Time Series analysis.
Prior to that, I completed my Bachelor of Engineering in Electronics and Communication at PSG College of Technology in Tamil Nadu, India, from June 2018 to May 2021, with a GPA of 7.5/10.0. My undergraduate coursework included Relational Database Management Systems, Data Structures and Algorithms, and IT & Software Fundamentals.
I hold an AWS Certified Cloud Practitioner certification and have completed the UDEMY Data Science Bootcamp.



How I built this AI: I built a chatbot using the Retrieval-Augmented Generation (RAG) pipeline to enable context-aware responses grounded in custom documents. I began by loading the relevant documents and splitting them into coherent chunks using `RecursiveCharacterTextSplitter`, which ensures semantic integrity by breaking the text along logical boundaries like paragraphs and sentences. These chunks were then embedded using OpenAI’s `text-embedding-ada-002` model to generate dense vector representations that capture semantic meaning. I stored these vectors in a FAISS (Facebook AI Similarity Search) vector store, which was ideal for this small-scale project due to its in-memory speed and efficient similarity search capabilities. When a user submits a query, the system retrieves the most relevant document chunks from FAISS and passes them, along with the query, to GPT-3.5 Turbo for final answer synthesis. This approach combines precise retrieval with the natural language generation capabilities of the LLM, ensuring the chatbot produces accurate, context-rich responses tailored to the user's input.

If asked anything about how can i replicate this and stuff please elaboarte on the topics which i used to create it and provide a guide.